{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 GraphRAG and AI Agent (Azure OpenAI Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU pymongo langchain_community langchain_openai langchain_mongodb voyageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import voyageai\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# 1. í™˜ê²½ ë³€ìˆ˜ ë° í‚¤ ì„¤ì •\n",
    "MONGODB_URI = \"\"\n",
    "VOYAGE_API_KEY = \"\"\n",
    "\n",
    "# Azure OpenAI ì„¤ì •\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n",
    "\n",
    "DB_NAME = \"workshop_db\"\n",
    "COLLECTION_NAME = \"books\"\n",
    "GRAPH_COLLECTION_NAME = \"books_graph\"\n",
    "VECTOR_INDEX_NAME = f\"{DB_NAME}_vector_search_index\"\n",
    "\n",
    "# 2. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "# Voyage AI\n",
    "vo = voyageai.Client(api_key=VOYAGE_API_KEY)\n",
    "\n",
    "# MongoDB\n",
    "mongodb_client = MongoClient(MONGODB_URI)\n",
    "db = mongodb_client[DB_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (From MongoDB)\n",
    "\n",
    "ì´ë¯¸ êµ¬ì¶•ë˜ì–´ ìˆëŠ” `books` ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ Vector Searchê°€ ë°”ë¼ë³´ëŠ” ë°ì´í„°ì™€ GraphRAGê°€ ë°”ë¼ë³´ëŠ” ë°ì´í„°ê°€ 100% ì¼ì¹˜í•¨ì„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# ê¸°ì¡´ books ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„° ì¡°íšŒ (ì¤„ê±°ë¦¬ê°€ ìˆëŠ” ì±…ë§Œ)\n",
    "cursor = db[COLLECTION_NAME].find({\"synopsis\": {\"$exists\": True, \"$ne\": \"\"}})\n",
    "\n",
    "documents = []\n",
    "for doc in cursor:\n",
    "    # GraphRAG ìƒì„±ì„ ìœ„í•œ ë¬¸ì„œ í¬ë§·íŒ…\n",
    "    content = f\"Title: {doc['title']}\\nSynopsis: {doc['synopsis']}\"\n",
    "    documents.append(Document(page_content=content, metadata={\"title\": doc['title']}))\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from existing '{COLLECTION_NAME}' collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GraphRAG êµ¬ì¶• (New Collection)\n",
    "\n",
    "ê¸°ì¡´ `books` ì»¬ë ‰ì…˜ì€ ê±´ë“œë¦¬ì§€ ì•Šê³ , `books_graph` ì»¬ë ‰ì…˜ì— ë…¸ë“œì™€ ì—£ì§€ ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_mongodb.graphrag.graph import MongoDBGraphStore\n",
    "\n",
    "# 1. LLM ì´ˆê¸°í™” (Azure OpenAI)\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-5.1-chat\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    ")\n",
    "\n",
    "# from_connection_stringì€ ì»¬ë ‰ì…˜ ìƒì„±ì„ ì‹œë„í•˜ë¯€ë¡œ, ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
    "if GRAPH_COLLECTION_NAME in db.list_collection_names():\n",
    "    print(f\"Dropping existing collection: {GRAPH_COLLECTION_NAME}\")\n",
    "    db[GRAPH_COLLECTION_NAME].drop()\n",
    "\n",
    "# 2. Graph Store ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ì‚¬ìš©)\n",
    "graph_store = MongoDBGraphStore.from_connection_string(\n",
    "    connection_string=MONGODB_URI,\n",
    "    database_name=DB_NAME,\n",
    "    collection_name=GRAPH_COLLECTION_NAME,\n",
    "    entity_extraction_model=llm,\n",
    ")\n",
    "\n",
    "# 3. ê·¸ë˜í”„ ë°ì´í„° ìƒì„±\n",
    "print(\"Building Knowledge Graph... (This involves LLM processing)\")\n",
    "graph_store.add_documents(documents)\n",
    "print(\"Graph construction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "FEW_SHOT_ENTITY_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an entity extractor. extract key entities (Person, Book, Concept, etc.) from the query.\n",
    "    \n",
    "    EXAMPLES:\n",
    "    Input: \"Who wrote Harry Potter?\"\n",
    "    Output: [\"Harry Potter\"]\n",
    "    \n",
    "    Input: \"books about magic and wizards\"\n",
    "    Output: [\"magic\", \"wizards\"]\n",
    "    \n",
    "    Input: \"What is the connection between A and B?\"\n",
    "    Output: [\"A\", \"B\"]\n",
    "\n",
    "    RULES:\n",
    "    1. Return ONLY a JSON list. \n",
    "    2. No markdown, no explanations.\n",
    "    \n",
    "    Input: \"{input}\"\n",
    "    Output:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "graph_collection = db[GRAPH_COLLECTION_NAME]\n",
    "\n",
    "# Graph Store ì¬ì—°ê²° (ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ì ìš©)\n",
    "existing_graph_store = MongoDBGraphStore(\n",
    "    collection=graph_collection,\n",
    "    entity_extraction_model=llm,\n",
    "    entity_prompt=FEW_SHOT_ENTITY_PROMPT  # <--- ì˜ˆì‹œê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "def run_graph_rag(query: str):\n",
    "    # ì—ëŸ¬ê°€ ë‚˜ë„ ì£½ì§€ ì•Šê³  ì›ì¸ì„ ë³´ì—¬ì£¼ë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€\n",
    "    try:\n",
    "        response = existing_graph_store.chat_response(query)\n",
    "        return f\"[GraphRAG Answer]\\n{response.content}\"\n",
    "    except Exception as e:\n",
    "        return f\"[GraphRAG Error]\\nLLM Output format error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Weston Hingler?\"\n",
    "# query = \"What connects 'The Design of Everyday Things' to the concept of 'Product Design'?\"\n",
    "# query = \"What do 'Ocean Sea' and 'Silk' have in common?\"\n",
    "# query = \"Which book features a character who is an 'Illusionist' or 'Magician'?\"\n",
    "# query = \"What books did J.R.R. Tolkien write?\"\n",
    "\n",
    "run_graph_rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyvis networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_graph(collection):\n",
    "    docs = list(collection.find())\n",
    "    \n",
    "    def format_attributes(attrs):\n",
    "        return \"<br>\".join(f\"{k}: {', '.join(v)}\" for k, v in attrs.items()) if attrs else \"\"\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Create nodes\n",
    "    for doc in docs:\n",
    "        node_id = str(doc[\"_id\"])\n",
    "        info = f\"Type: {doc.get('type', '')}\"\n",
    "        if \"attributes\" in doc:\n",
    "            attr_info = format_attributes(doc[\"attributes\"])\n",
    "            if attr_info:\n",
    "                info += \"<br>\" + attr_info\n",
    "        G.add_node(node_id, label=node_id, title=info.replace(\"<br>\", \"\\n\"))\n",
    "\n",
    "    # Create edges\n",
    "    for doc in docs:\n",
    "        source = str(doc[\"_id\"])\n",
    "        rels = doc.get(\"relationships\", {})\n",
    "        targets = rels.get(\"target_ids\", [])\n",
    "        types = rels.get(\"types\", [])\n",
    "        attrs = rels.get(\"attributes\", [])\n",
    "        \n",
    "        for i, target in enumerate(targets):\n",
    "            edge_type = types[i] if i < len(types) else \"\"\n",
    "            extra = attrs[i] if i < len(attrs) else {}\n",
    "            edge_info = f\"Relationship: {edge_type}\"\n",
    "            if extra:\n",
    "                edge_info += \"<br>\" + format_attributes(extra)\n",
    "            G.add_edge(source, str(target), label=edge_type, title=edge_info.replace(\"<br>\", \"\\n\"))\n",
    "\n",
    "    # Build and configure network\n",
    "    nt = Network(notebook=True, cdn_resources='in_line', width=\"800px\", height=\"600px\", directed=True)\n",
    "    nt.from_nx(G)\n",
    "    nt.set_options('''\n",
    "    var options = {\n",
    "      \"interaction\": {\n",
    "        \"hover\": true,\n",
    "        \"tooltipDelay\": 200\n",
    "      },\n",
    "      \"nodes\": {\n",
    "        \"font\": {\"multi\": \"html\"}\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"repulsion\": {\n",
    "          \"nodeDistance\": 300,\n",
    "          \"centralGravity\": 0.2,\n",
    "          \"springLength\": 200,\n",
    "          \"springStrength\": 0.05,\n",
    "          \"damping\": 0.09\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    ''')\n",
    "\n",
    "    return nt.generate_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "collection = db[GRAPH_COLLECTION_NAME]\n",
    "html = visualize_graph(collection)\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë¹„êµë¥¼ ìœ„í•œ ì¤€ë¹„: í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ê°€ (Text Embedding)\n",
    "\n",
    "ìš°ë¦¬ëŠ” Part 1ì—ì„œ ì±… í‘œì§€(ì´ë¯¸ì§€)ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
    "í•˜ì§€ë§Œ ì§€ê¸ˆ ìš°ë¦¬ê°€ í•˜ë ¤ëŠ” ë¹„êµ ì‹¤í—˜ì€ \"ì¤„ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‚´ìš©ì„ ë¬»ëŠ” ê²ƒ\"ì…ë‹ˆë‹¤.\n",
    "ì´ë¯¸ì§€ ë²¡í„°ë¡œëŠ” ì¤„ê±°ë¦¬ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë¹„êµê°€ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ, ê¸°ì¡´ ì´ë¯¸ì§€ ë²¡í„°(`embedding`)ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , ì¤„ê±°ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•˜ì—¬ `text_embedding`ì´ë¼ëŠ” ìƒˆë¡œìš´ í•„ë“œì— ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Generating Text Embeddings for Fair Comparison...\")\n",
    "\n",
    "# 1. ì¤„ê±°ë¦¬ê°€ ìˆëŠ” ì±…ë“¤ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "cursor = db[COLLECTION_NAME].find({\"synopsis\": {\"$exists\": True, \"$ne\": \"\"}})\n",
    "docs_to_update = list(cursor)\n",
    "\n",
    "# 2. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ë° 'text_embedding' í•„ë“œì— ì €ì¥\n",
    "for doc in tqdm(docs_to_update, desc=\"Updating text_embedding\"):\n",
    "    # GraphRAGì™€ ë™ì¼í•œ ì†ŒìŠ¤ í…ìŠ¤íŠ¸ ì‚¬ìš©\n",
    "    text_content = f\"Title: {doc['title']}\\nSynopsis: {doc['synopsis']}\"\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (Voyage AI)\n",
    "    new_embedding = vo.multimodal_embed(\n",
    "        inputs=[[text_content]], \n",
    "        model=\"voyage-multimodal-3\", \n",
    "        input_type=\"document\"\n",
    "    ).embeddings[0]\n",
    "    \n",
    "    # DB ì—…ë°ì´íŠ¸: ê¸°ì¡´ 'embedding'ì€ ê±´ë“œë¦¬ì§€ ì•Šê³  'text_embedding' í•„ë“œë¥¼ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "    db[COLLECTION_NAME].update_one(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$set\": {\"text_embedding\": new_embedding}}\n",
    "    )\n",
    "\n",
    "print(\"Text embeddings saved to 'text_embedding' field!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒˆë¡œìš´ í•„ë“œ(text_embedding)ê°€ ìƒê²¼ìœ¼ë‹ˆ, ì´ í•„ë“œë¥¼ ì „ìš©ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì¸ë±ìŠ¤(vector_index_text)ê°€ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_index, check_index_ready\n",
    "\n",
    "TEXT_VECTOR_INDEX_NAME = f\"{DB_NAME}_vector_index_text\"\n",
    "\n",
    "# ì¸ë±ìŠ¤ ì •ì˜ (pathê°€ 'text_embedding'ì¸ ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤)\n",
    "text_vector_index_model = {\n",
    "    \"name\": TEXT_VECTOR_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"text_embedding\",\n",
    "                \"numDimensions\": 1024,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# `create_index` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `collection` ì»¬ë ‰ì…˜ì— ìœ„ ì •ì˜ëŒ€ë¡œ ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "create_index(collection, TEXT_VECTOR_INDEX_NAME, text_vector_index_model)\n",
    "\n",
    "# ì§„í–‰í•˜ê¸° ì „ì— `check_index_ready` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ê°€ ìƒì„±ë˜ì—ˆê³  'READY' ìƒíƒœì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "check_index_ready(collection, TEXT_VECTOR_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë¹„êµ ì‹¤í—˜: Vector Search vs GraphRAG\n",
    "\n",
    "ì´ì œ ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì„ ë¹„êµí•´ ë´…ë‹ˆë‹¤.\n",
    "* **Vector Search:** Part 1ì—ì„œ ë§Œë“  `books` ì»¬ë ‰ì…˜ + `vector_index` ì‚¬ìš©\n",
    "* **GraphRAG:** ë°©ê¸ˆ ë§Œë“  `books_graph` ì»¬ë ‰ì…˜ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Standard RAG] Vector Search ê²°ê³¼ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "def run_standard_rag(query: str):\n",
    "    # 1. ì„ë² ë”© ìƒì„± (Voyage AI)\n",
    "    query_embedding = vo.multimodal_embed(\n",
    "        inputs=[[query]], model=\"voyage-multimodal-3\", input_type=\"query\"\n",
    "    ).embeddings[0]\n",
    "\n",
    "    # 2. Vector Search ì‹¤í–‰ (Top 3)\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": TEXT_VECTOR_INDEX_NAME,\n",
    "                \"path\": \"text_embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": 50,\n",
    "                \"limit\": 3\n",
    "            }\n",
    "        },\n",
    "        {\"$project\": {\"_id\": 0, \"title\": 1, \"synopsis\": 1}}\n",
    "    ]\n",
    "    results = list(db[COLLECTION_NAME].aggregate(pipeline))\n",
    "\n",
    "    # 3. ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°\n",
    "    if not results:\n",
    "        return \"[Standard RAG Answer]\\nI couldn't find any relevant documents.\"\n",
    "\n",
    "    # 4. Context êµ¬ì„± (ê²€ìƒ‰ëœ ì¤„ê±°ë¦¬ í•©ì¹˜ê¸°)\n",
    "    context_text = \"\"\n",
    "    for doc in results:\n",
    "        context_text += f\"Title: {doc['title']}\\nSynopsis: {doc.get('synopsis', 'No synopsis available')}\\n\\n\"\n",
    "\n",
    "    # 5. LLMì—ê²Œ ë‹µë³€ ìš”ì²­ (Azure OpenAI)\n",
    "    # GraphRAGì™€ ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ ë™ì¼í•œ LLM(llm ê°ì²´)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "    If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return f\"[Standard RAG Answer]\\n{response.content}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¹„êµ ì‹œë‚˜ë¦¬ì˜¤: ë³µí•©ì ì¸ ê´€ê³„ ì¶”ë¡ \n",
    "ë²¡í„° ê²€ìƒ‰ì€ í‚¤ì›Œë“œ ë§¤ì¹­ì—ëŠ” ê°•í•˜ì§€ë§Œ, \"Aì™€ Bì˜ ê´€ê³„\"ë‚˜ \"íŠ¹ì • ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” ì±…ì˜ ê°ˆë“±\" ê°™ì€ ì§ˆë¬¸ì—ëŠ” ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [ìµœì¢… ë¹„êµ ì‹¤í–‰] ---\n",
    "# query = \"Who is Weston Hingler?\"\n",
    "# query = \"What connects 'The Design of Everyday Things' to the concept of 'Product Design'?\"\n",
    "# query = \"What do 'Ocean Sea' and 'Silk' have in common?\"\n",
    "# query = \"Which book features a character who is an 'Illusionist' or 'Magician'?\"\n",
    "# query = \"What books did J.R.R. Tolkien write?\"\n",
    "query = \"Who wrote the books The Hobbit?\"\n",
    "# query = \"What businesses are owned by Weston Hingler's parents?\"\n",
    "# query = \"Give me a summary of 'Prodigal Summer'.\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"QUERY: {query}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Standard RAG (Text Vector)\n",
    "print(run_standard_rag(query))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. GraphRAG (Graph)\n",
    "print(run_graph_rag(query)) # ì•ì„œ ì •ì˜í•œ run_graph_rag ì¬ì‚¬ìš©\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me a summary of 'Prodigal Summer'.\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"QUERY: {query}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Standard RAG (Text Vector)\n",
    "# print(run_standard_rag(query))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. GraphRAG (Graph)\n",
    "print(run_graph_rag(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AI Agent (GraphRAG Tool í™œìš©)\n",
    "\n",
    "GraphRAGë¥¼ ì—ì´ì „íŠ¸ì˜ ë„êµ¬ë¡œ ì¥ì°©í•˜ì—¬, í•„ìš”í•  ë•Œ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ì¡°íšŒí•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Annotated, TypedDict, List\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# 1. ë„êµ¬(Tools) ì •ì˜\n",
    "\n",
    "# Azure OpenAI LLM ì •ì˜\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-5.1-chat\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    ")\n",
    "\n",
    "@tool\n",
    "def vector_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Useful for answering questions about specific plot details, summaries, text excerpts, \n",
    "    or general descriptions of books. Use this for 'What is the book about?' type questions.\n",
    "    \"\"\"\n",
    "    # ì•ì„œ ì •ì˜í•œ Standard RAG í•¨ìˆ˜ ì¬ì‚¬ìš©\n",
    "    return run_standard_rag(query)\n",
    "\n",
    "@tool\n",
    "def graph_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Useful for answering questions about relationships between characters, authors, \n",
    "    business ownership, family trees, or specific connections. \n",
    "    Use this for 'Who is related to whom?' or 'Who owns what?' type questions.\n",
    "    \"\"\"\n",
    "    # ì•ì„œ ì •ì˜í•œ Graph RAG í•¨ìˆ˜ ì¬ì‚¬ìš©\n",
    "    return run_graph_rag(query)\n",
    "\n",
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸\n",
    "tools = [vector_search_tool, graph_search_tool]\n",
    "\n",
    "# 2. LLMì— ë„êµ¬ ë°”ì¸ë”© (Bind Tools)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 3. ê·¸ë˜í”„ ìƒíƒœ(State) ì •ì˜\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List, add_messages]\n",
    "\n",
    "# 4. ë…¸ë“œ(Nodes) ì •ì˜\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"í˜„ì¬ ìƒíƒœë¥¼ ë³´ê³  LLMì´ ë‹µë³€í• ì§€ ë„êµ¬ë¥¼ í˜¸ì¶œí• ì§€ ê²°ì •\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ (LangGraph Prebuilt ì‚¬ìš©)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 5. ì¡°ê±´ë¶€ ì—£ì§€(Conditional Edge) ì •ì˜\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"LLMì˜ ì‘ë‹µì— tool_callsê°€ ìˆìœ¼ë©´ ë„êµ¬ ì‹¤í–‰ ë…¸ë“œë¡œ, ì—†ìœ¼ë©´ ì¢…ë£Œ\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# 6. ê·¸ë˜í”„ êµ¬ì¶• (Build Graph)\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# ì—£ì§€ ì—°ê²°\n",
    "workflow.set_entry_point(\"agent\") # ì‹œì‘ -> ì—ì´ì „íŠ¸\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\") # ë„êµ¬ ì‹¤í–‰ í›„ -> ë‹¤ì‹œ ì—ì´ì „íŠ¸(ê²°ê³¼ ì¢…í•©)\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def run_agent_test(query: str):\n",
    "    \"\"\"\n",
    "    ì—ì´ì „íŠ¸ì—ê²Œ ì§ˆë¬¸ì„ ë˜ì§€ê³ , ì–´ë–¤ ë„êµ¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ì™€ ìµœì¢… ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ—£ï¸ User Question: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    final_response = None\n",
    "\n",
    "    # LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰\n",
    "    # messages í‚¤ì— HumanMessageë¥¼ ë‹´ì•„ì„œ ê·¸ë˜í”„ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    for event in app.stream({\"messages\": [HumanMessage(content=query)]}):\n",
    "        \n",
    "        for node_name, state in event.items():\n",
    "            # 1. ì—ì´ì „íŠ¸ ë…¸ë“œ: ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ íŒë‹¨\n",
    "            if node_name == \"agent\":\n",
    "                last_msg = state[\"messages\"][-1]\n",
    "                if last_msg.tool_calls:\n",
    "                    tool_name = last_msg.tool_calls[0]['name']\n",
    "                    print(f\"ğŸ¤– Agent Thought: 'I need to use [{tool_name}] to answer this.'\")\n",
    "                    print(f\"ğŸ‘‰ Calling Tool: {tool_name}...\")\n",
    "            \n",
    "            # 2. ë„êµ¬ ë…¸ë“œ: ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ\n",
    "            elif node_name == \"tools\":\n",
    "                print(\"ğŸ”§ Tool Output: Data received successfully.\")\n",
    "\n",
    "            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì¶”ì  (ìµœì¢… ë‹µë³€ ì¶œë ¥ì„ ìœ„í•´)\n",
    "            if \"messages\" in state:\n",
    "                final_response = state[\"messages\"][-1].content\n",
    "\n",
    "    # ìµœì¢… ë‹µë³€ ì¶œë ¥\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"ğŸ’¡ Final Answer:\\n{final_response}\")\n",
    "    print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [í…ŒìŠ¤íŠ¸ 1] ì¤„ê±°ë¦¬/ìš”ì•½ ì§ˆë¬¸ -> Vector Search ê¸°ëŒ€\n",
    "# \"Prodigal Summer\"ì˜ ì¤„ê±°ë¦¬ëŠ” í…ìŠ¤íŠ¸ì—ë§Œ ìˆìœ¼ë¯€ë¡œ vector_search_toolì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "run_agent_test(\"Give me a summary of 'Prodigal Summer'.\")\n",
    "\n",
    "# [í…ŒìŠ¤íŠ¸ 2] ê´€ê³„/êµ¬ì¡° ì§ˆë¬¸ -> Graph Search ê¸°ëŒ€\n",
    "# \"Weston Hingler\"ì˜ ë¶€ëª¨ë‹˜ì´ ì†Œìœ í•œ ì‚¬ì—…ì²´ëŠ” ê·¸ë˜í”„ ê´€ê³„ì— ìˆìœ¼ë¯€ë¡œ graph_search_toolì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "run_agent_test(\"What businesses are owned by Weston Hingler's parents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n8n ì‹¤ìŠµì„ ìœ„í•œ Vector Search Index ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N8N_VECTOR_INDEX_NAME = f\"{DB_NAME}_n8n\"\n",
    "\n",
    "model = {\n",
    "    \"name\": N8N_VECTOR_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 1536,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "collection = db[\"inven\"]\n",
    "\n",
    "create_index(collection, N8N_VECTOR_INDEX_NAME, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
